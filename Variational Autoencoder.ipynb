{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Variational Inference**\n",
    "Variational inference is a method for approximating the posterior distribution of some variables $x$ given data $D$. For example $x$ could be natural images belonging to different classes and $D$ is then dataset that we use to build a model. Using Bayes rule we can write the posterior as $p(x|D) = p(D|x)p(x)/P(D)$. When the prior $p(x)$ is conjugate to the likelihood $p(D|x)$ we can calculate this exactly but in generally  we cannot always find this analytically because the normalizing constant $p(D)$ is intractable to compute. \n",
    "\n",
    "The key idea in variational inference is to pick another distribution $q(x)$ and optimise its parameters until it is similar to the posterior. We usually measure similarity using the KL divergence between q and p. \n",
    "\n",
    "- Explain lower bound\n",
    "    - Two terms in LB: 1/KL of q(x) and p(x|D), 2/constant\n",
    "    - We can't calculate the two terms individually - that is after all why we need an approximation \n",
    "    - But we can implicitly maximise the KL-divergence by maximising the LB\n",
    "          \n",
    "- Explain mean field method\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use neural networks as probabilistic encoders and decoders. We assume that both $p(\\mathbf{x}|\\mathbf{z})$ and $p(\\mathbf{z}|\\mathbf{x})$ are multivariate Gaussian distributions with a diagonal covariance. The encoder is a neural network which takes as input examples from our training set $\\mathbf{x}$ and outputs the mean and log of the variance of the $p(\\mathbf{z}|\\mathbf{x})$. These parameters are used to convert samples drawn from the noise distribution $N(\\mathbf{0},\\mathbf{I})$, $\\mathbf{\\epsilon}$, into samples, $\\mathbf{z}$, from $p(\\mathbf{z}|\\mathbf{x})$. These are then input to the decoder neural network which gives us the paramaters for $p(\\mathbf{x}|\\mathbf{z})$. This process is illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=VAE.svg style=\"width: 50%; height:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the diagram suggests that we can draw $L$ samples, where $L > 1$, from $p(\\mathbf{z}|\\mathbf{x})$ per datapoint $\\mathbf{x}$ by drawing $L\\times M$ samples $\\mathbf{\\epsilon}$ from $N(0,I)$ where $M$ is the number of training examples.\n",
    "\n",
    "We will start off by implementing the simple, shallow network described in the paper, with just one hidden layer in each of the encoder and decoder. The function below takes a batch of input vectors and outputs two vectors each input vector, which we interpret as the parameters of a Gaussian distribution conditioned on the input. We can easily modify the architecture later for example to make it deeper or to use convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_module(x, name, units):\n",
    "    #Add initializers\n",
    "    with tf.variable_scope(name):\n",
    "        h = tf.layers.dense(inputs=x, units = units[0], activation=tf.tanh, name='h')\n",
    "        print('h', h.shape)\n",
    "        mu = tf.layers.dense(inputs=h, units = units[1], \n",
    "            activation = None, name='mu')\n",
    "        log_sig2 = tf.layers.dense(inputs=h, units = units[1], activation = None, name='log_sig2')\n",
    "    print('mu_f', mu.shape, 'log_sig2_f', log_sig2.shape)\n",
    "    return mu, log_sig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will be trained to minimise the following cost function:\n",
    "\n",
    "$$\\mathrm{L}(\\theta, \\phi; \\mathbf{x}^{(i)}) = -D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}^{(i)})||p_\\theta(\\mathbf{z}))\n",
    "+ \\frac{1}{L}\\sum\\limits_{l=1}^{L}\\log p_\\theta(\\mathbf{x}^{(i)}|\\mathbf{z}^{(i,l)})$$\n",
    "\n",
    "As discussed earlier, it is possible to evaluate exactly the term $D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}^{(i)})||p_\\theta(\\mathbf{z}))$ in some cases. In the present situation, where we are dealing with multivariate Gaussian distributions, this term turns to be:\n",
    "\n",
    "$$\\frac{1}{2}\\sum\\limits_{j=1}^{J}(1 + \\log((\\sigma_j^{(i)})^2) + (\\mu_j^{(i)})^2 + (\\sigma_j^{(i)})^2)$$\n",
    "\n",
    "[What is $J$ above???]\n",
    "\n",
    "The paper provides a derivation of this result but I have included a more detailed version here [???].\n",
    "\n",
    "The following functions calculate the KL-divergence and the estimate of $E_{q_\\phi(\\mathbf{z}|\\mathbf{x}^{(i)})}\\left[\\log p_\\theta(\\mathbf{x}^{(i)}|\\mathbf{z})\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kl_div(mu, log_sig2):\n",
    "    log_kl = 1 + log_sig2 - mu**2 - tf.exp(log_sig2)\n",
    "    print('log_kl', log_kl.shape)\n",
    "    return 0.5*tf.reduce_sum(log_kl, axis=-1)#ascertain axis\n",
    "\n",
    "def log_prob(x, log_sig_2, mu):\n",
    "    lp = -(tf.reduce_sum(tf.ones_like(log_sig_2)*np.pi + log_sig_2, axis=-1) \n",
    "      + tf.reduce_sum((x-mu)**2, axis=-1)\n",
    "      + tf.reduce_sum(1./tf.exp(log_sig_2), axis=-1))/2\n",
    "    print('lp', lp.shape)\n",
    "    lp = tf.reduce_mean(lp, axis=-1)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can assemble the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify the dimensions \n",
    "xdim = 64\n",
    "zdim = 10\n",
    "hu = 200\n",
    "\n",
    "x_shape = [xdim]*2\n",
    "z_shape = [zdim]\n",
    "\n",
    "M = 32\n",
    "L = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (100, 64, 64) eps (100, 7, 10)\n",
      "h (100, 200)\n",
      "mu_f (100, 10) log_sig2_f (100, 10)\n",
      "mu_z_x (100, 1, 10) log_sig2_z_x (100, 1, 10)\n",
      "z (100, 7, 10)\n",
      "h (700, 200)\n",
      "mu_f (700, 4096) log_sig2_f (700, 4096)\n",
      "mu_x_z (700, 4096) log_sig2_x_z (700, 4096)\n",
      "log_kl (100, 10)\n",
      "kl_term (100,)\n",
      "mu_x_z (100, 7, 4096) log_sig2_x_z (100, 7, 4096)\n",
      "x_expanded (100, 1, 4096)\n",
      "lp (100, 7)\n",
      "mc_term (100,)\n",
      "lower_bound ()\n"
     ]
    }
   ],
   "source": [
    "# def vae(x_shape, z_shape, M=100, L=1):\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#PLACEHOLDERS\n",
    "x = tf.placeholder(dtype=tf.float32, name='x', shape=[M]+x_shape)\n",
    "eps = tf.placeholder(dtype=tf.float32, name='x', shape=[M, L]+z_shape)\n",
    "\n",
    "#ENCODER\n",
    "x_flat = tf.reshape(x, [M,-1])\n",
    "print('x', x.shape, 'eps', eps.shape)\n",
    "params_z_x = vae_module(x_flat, 'encoder', [200, np.product(z_shape)])\n",
    "##Make the parameters M x 1 x zdim to enable broadcasting \n",
    "##when multiplying with eps which has dimensions M x L x zdim\n",
    "log_sig2_z_x, mu_z_x = [tf.expand_dims(param, axis=1) for param in params_z_x]\n",
    "print('mu_z_x', mu_z_x.shape, 'log_sig2_z_x', log_sig2_z_x.shape)\n",
    "\n",
    "#SAMPLE FROM P(Z|X) USING REPARAMETERISATION TRICK'\n",
    "z = mu_z_x + log_sig2_z_x*eps\n",
    "print('z', z.shape)\n",
    "\n",
    "#DECODER\n",
    "##Flatten z to use in decoder\n",
    "z_flat = tf.reshape(z, [M*L,-1])\n",
    "log_sig2_x_z, mu_x_z = vae_module(z_flat, 'decoder', [200, np.product(x_shape)])\n",
    "# log_sig2_x_z, mu_x_z = [tf.reshape(param, [M, L, -1]) for param in params_x_z]\n",
    "print('mu_x_z', mu_x_z.shape, 'log_sig2_x_z', log_sig2_x_z.shape)\n",
    "log_sig2_z_x, mu_z_x = [tf.squeeze(param, axis=1) for param in [log_sig2_z_x, mu_z_x]]\n",
    "\n",
    "#KL TERM\n",
    "kl_term = gaussian_kl_div(mu_z_x, log_sig2_z_x)\n",
    "print('kl_term', kl_term.shape)\n",
    "\n",
    "#MC TERM\n",
    "log_sig2_x_z, mu_x_z = [tf.reshape(param, [M, L, -1]) for param in [log_sig2_x_z, mu_x_z ]]\n",
    "print('mu_x_z', mu_x_z.shape, 'log_sig2_x_z', log_sig2_x_z.shape)\n",
    "##Expand x_flat as we expanded the parameters of p(z|x) above to allow broadcasting\n",
    "x_expanded = tf.expand_dims(x_flat, axis=1)\n",
    "print('x_expanded', x_expanded.shape)\n",
    "mc_term = log_prob(x_expanded, mu_x_z, log_sig2_x_z)\n",
    "print('mc_term', mc_term.shape)\n",
    "\n",
    "#LOSS\n",
    "lower_bound = tf.reduce_mean(mc_term - kl_term)\n",
    "print('lower_bound', lower_bound.shape)\n",
    "\n",
    "#OPTIMIZE\n",
    "optim = tf.train.GradientDescentOptimizer(1e-2).minimize(lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.session() as sess:\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(math.ceil(num_examples/batch_size)):\n",
    "            batch = data[j:j+batch_size]\n",
    "            noise_samples = np.random.normal(loc=0,scale=1,size=[M, L]+z_shape)\n",
    "            logvar_xz, mu_xz, lb = sess.run(feed_dict={x: batch, eps: noise_samples},\n",
    "                                                             fetch=[log_sig2_x_z, mu_x_z, lower_bound])\n",
    "        mu = mu_xz.reshape(-1)\n",
    "        cov = np.diag(np.exp(logvar_xz).reshape(-1))\n",
    "        images = np.random.multivariate_normal(mean=mu, cov=cov, size=5)\n",
    "        images.reshape([xdim, -1]) #Assuming samples are of form [image, image, ..., image], image is 64 x 64\n",
    "        plt.subplot(num_epochs, 1, i+1)\n",
    "        plt.imshow(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
